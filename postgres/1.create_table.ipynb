{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers psycopg2 numpy boto3 torch scikit-learn matplotlib nltk sentence-transformers pandas langchain lark pgvector psycopg2-binary tiktoken langchain_community huggingface_hub replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import Pool\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import psycopg2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"bart\": {\n",
    "        \"model_name\": \"facebook/bart-large\",\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained(\"facebook/bart-large\", trust_remote_code=True),\n",
    "        \"model\": AutoModel.from_pretrained(\"facebook/bart-large\", trust_remote_code=True)\n",
    "    },\n",
    "    \"gte\": {\n",
    "        \"model_name\": \"Alibaba-NLP/gte-large-en-v1.5\",\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-large-en-v1.5\", trust_remote_code=True),\n",
    "        \"model\": AutoModel.from_pretrained(\"Alibaba-NLP/gte-large-en-v1.5\", trust_remote_code=True)\n",
    "    },\n",
    "    \"MiniLM\": {\n",
    "        \"model_name\": 'all-MiniLM-L12-v2',\n",
    "        \"model\": SentenceTransformer('all-MiniLM-L12-v2')\n",
    "    },\n",
    "    \"roberta\": {\n",
    "        \"model_name\": 'sentence-transformers/nli-roberta-large',\n",
    "        \"model\": SentenceTransformer('sentence-transformers/nli-roberta-large')\n",
    "    },\n",
    "    \"e5-large\":{\n",
    "        \"model_name\": 'intfloat/e5-large',\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained('intfloat/e5-large', trust_remote_code=True),\n",
    "        \"model\": AutoModel.from_pretrained('intfloat/e5-large', trust_remote_code=True)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "with open(os.path.join(current_directory, \"movies.json\"), \"r\") as f:\n",
    "    movies = json.load(f)\n",
    "\n",
    "movies_data = []\n",
    "for movie in movies[\"films\"][\"film\"]:\n",
    "\n",
    "    roles = movie.get(\"role\", [])\n",
    "    if isinstance(roles, dict):  # If 'roles' is a dictionary, make it a single-item list\n",
    "        roles = [roles]\n",
    "\n",
    "    # Extract actor information\n",
    "    actors = []\n",
    "    for role in roles:\n",
    "        actor_info = role.get(\"acteur\", {})\n",
    "        if \"__text\" in actor_info:\n",
    "            actors.append(actor_info[\"__text\"])\n",
    "\n",
    "    movies_data.append({\n",
    "        \"title\": movie.get(\"titre\", \"\"),\n",
    "        \"year\": movie.get(\"annee\", \"\"),\n",
    "        \"country\": movie.get(\"pays\", \"\"),\n",
    "        \"language\": movie.get(\"langue\", \"\"),\n",
    "        \"duration\": movie.get(\"duree\", \"\"),\n",
    "        \"summary\": movie.get(\"synopsis\", \"\"),\n",
    "        \"genre\": movie.get(\"genre\", \"\"),\n",
    "        \"director\": movie.get(\"realisateur\", {\"__text\": \"\"}).get(\"__text\", \"\"),\n",
    "        \"writers\": movie.get(\"scenariste\", []),\n",
    "        \"actors\": actors,\n",
    "        \"poster\": movie.get(\"affiche\", \"\"),\n",
    "        \"id\": movie.get(\"id\", \"\")\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Example preprocessing step simplified for demonstration\n",
    "    tokens = text.split()\n",
    "    # Assuming stopwords are already loaded to avoid loading them in each process\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stopwords_set]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    \"\"\" Normalize the embeddings to unit vectors. \"\"\"\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    normalized_embeddings = embeddings / norms\n",
    "    return normalized_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(movies_data, model_key, normalize=True):\n",
    "    model_config = models[model_key]\n",
    "    if 'tokenizer' in model_config:\n",
    "        # Handle HuggingFace transformer models\n",
    "        movie_texts = [\n",
    "            f\"{preprocess(movie['title'])} {movie['year']} {' '.join(movie['genre'])} \"\n",
    "            f\"{' '.join(movie['actors'])} {movie['director']} \"\n",
    "            f\"{preprocess(movie['summary'])} {movie['country']}\"\n",
    "            for movie in movies_data\n",
    "        ]\n",
    "        inputs = model_config['tokenizer'](movie_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model_config['model'](**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "    else:\n",
    "        # Handle Sentence Transformers\n",
    "        movie_texts = [\n",
    "            f\"{preprocess(movie['title'])} {movie['year']} {' '.join(movie['genre'])} \"\n",
    "            f\"{' '.join(movie['actors'])} {movie['director']} \"\n",
    "            f\"{preprocess(movie['summary'])} {movie['country']}\"\n",
    "            for movie in movies_data\n",
    "        ]\n",
    "        embeddings = model_config['model'].encode(movie_texts)\n",
    "\n",
    "    if normalize:\n",
    "        embeddings = normalize_embeddings(embeddings)\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_bart = generate_embedding(movies_data, 'bart')\n",
    "embeddings_bart = np.array(embeddings_bart)\n",
    "print(\"BART embeddings shape:\", embeddings_bart.shape)\n",
    "print(\"BART embeddings:\", embeddings_bart[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_gte = generate_embedding(movies_data, 'gte')\n",
    "embeddings_gte = np.array(embeddings_gte)\n",
    "print(\"GTE embeddings shape:\", embeddings_gte.shape)\n",
    "print(\"GTE embeddings:\", embeddings_gte[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_MiniLM = generate_embedding(movies_data, 'MiniLM')\n",
    "embeddings_MiniLM = np.array(embeddings_MiniLM)\n",
    "print(\"MiniLM embeddings shape:\", embeddings_MiniLM.shape)\n",
    "print(\"MiniLM embeddings:\", embeddings_MiniLM[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_roberta = generate_embedding(movies_data, 'roberta')\n",
    "embeddings_roberta = np.array(embeddings_roberta)\n",
    "print(\"RoBERTa embeddings shape:\", embeddings_roberta.shape)\n",
    "print(\"RoBERTa embeddings:\", embeddings_roberta[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_e5_large = generate_embedding(movies_data, 'e5-large')\n",
    "embeddings_e5_large = np.array(embeddings_e5_large)\n",
    "print(\"e5-large embeddings shape:\", embeddings_e5_large.shape)\n",
    "print(\"e5-large embeddings:\", embeddings_e5_large[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create connection to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(database=\"admin\", host=\"localhost\", user=\"admin\", password=\"admin\", port=\"5432\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "conn.commit()\n",
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS cube;\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_database():\n",
    "    cur.execute('DROP TABLE IF EXISTS movies')\n",
    "    cur.execute('''\n",
    "        CREATE TABLE movies (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            title TEXT NOT NULL,\n",
    "            actors TEXT,\n",
    "            year INTEGER,\n",
    "            country TEXT,\n",
    "            language TEXT,\n",
    "            duration INTEGER,\n",
    "            summary TEXT,\n",
    "            genre TEXT[],\n",
    "            director TEXT,\n",
    "            scenarists TEXT[],\n",
    "            poster TEXT,\n",
    "            embedding_bart VECTOR(1024),\n",
    "            embedding_gte VECTOR(1024),\n",
    "            embedding_MiniLM VECTOR(384),\n",
    "            embedding_roberta VECTOR(1024),\n",
    "            embedding_e5_large VECTOR(1024)\n",
    "        );\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "setup_database()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_movies(movie_data, embeddings_bart, embeddings_gte, embeddings_MiniLM, embeddings_roberta, embeddings_e5_large):\n",
    "    for movie, emb_bart, emb_gte, emb_MiniLM , emb_roberta, emb_e5_large in zip(movie_data, embeddings_bart, embeddings_gte, embeddings_MiniLM, embeddings_roberta, embeddings_e5_large):\n",
    "        # Joining actors into a single string separated by commas\n",
    "        actor_names = ', '.join(movie['actors'])\n",
    "        # Convert list of genres into a PostgreSQL array format\n",
    "        genre_array = '{' + ', '.join([f'\"{g}\"' for g in movie['genre']]) + '}'\n",
    "        # Convert list of scenarists into a PostgreSQL array format\n",
    "        scenarist_array = '{' + ', '.join([f'\"{s}\"' for s in movie['writers']]) + '}'\n",
    "        # Convert embeddings to a string properly formatted as a list\n",
    "        embedding_bart_str = '[' + ', '.join(map(str, emb_bart)) + ']'\n",
    "        embedding_gte_str = '[' + ', '.join(map(str, emb_gte)) + ']'\n",
    "        embedding_MiniLM_str = '[' + ', '.join(map(str, emb_MiniLM)) + ']'\n",
    "        embedding_roberta_str = '[' + ', '.join(map(str, emb_roberta)) + ']'\n",
    "        embedding_e5_large_str = '[' + ', '.join(map(str, emb_e5_large)) + ']'\n",
    "\n",
    "        cur.execute('''\n",
    "            INSERT INTO movies (title, actors, year, country, language, duration, summary, genre, director, scenarists, poster, embedding_bart, embedding_gte, embedding_MiniLM, embedding_roberta, embedding_e5_large)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        ''', (\n",
    "            movie['title'], actor_names, movie['year'], movie['country'], movie['language'],\n",
    "            movie['duration'], movie['summary'], genre_array, movie['director'],\n",
    "            scenarist_array, movie['poster'], embedding_bart_str, embedding_gte_str, embedding_MiniLM_str, embedding_roberta_str, embedding_e5_large_str\n",
    "        ))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_movies(movies_data, embeddings_bart, embeddings_gte, embeddings_MiniLM, embeddings_roberta, embeddings_e5_large)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
